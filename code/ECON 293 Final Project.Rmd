---
title: "Machine Learning and Causal Inference Final Project"
author: "Lucy Caffrey-Maffei"
date: "5/12/2022"
output:
  word_document: default
  html_document:
    df_print: paged
  pdf_document: default
---

# Setup

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr)
library(haven)
library(splines)
library(glmnet)
library(grf)
library(recipes)
library(fastDummies)
library(haven)
library(pacman)
library(readr)
library(janitor)
library(pacman)
p_load(labelled, plm, sandwich, lmtest, glue)

# Set working directory
options(stringAsFactors=FALSE)


star <- read_sav("~/Documents/Stanford/ECON 293 -  Machine Learning & Causal Inference/Econ293_Final_Project/Untitled/data/STAR_Students.sav")
names(star) <- tolower(names(star))

# creating indicator variable for treatment group; 1 = in small class for at least a year in grades kinder - 3rd
star <- star %>% 
  mutate(W = if_else(gkclasstype==1 | g1classtype==1 | g2classtype==1 | g3classtype==1, 1, 0)) %>% 
  mutate(W = if_else(is.na(W), 0, W)) 

# creating indicators for what year students were in small classes. 1 = in small class, 0 = not in small class
star$gk_smallclass <- case_when(star$gkclasstype == 1 ~ 1,
                                ((star$gkclasstype == 2) | (star$gkclasstype == 3)) ~ 0)

star$g1_smallclass <- case_when(star$g1classtype == 1 ~ 1,
                                ((star$g1classtype == 2) | (star$g1classtype == 3)) ~ 0)

star$g2_smallclass <- case_when(star$g2classtype == 1 ~ 1,
                                ((star$g2classtype == 2) | (star$g2classtype == 3)) ~ 0)

star$g3_smallclass <- case_when(star$g3classtype == 1 ~ 1,
                                ((star$g3classtype == 2) | (star$g3classtype == 3)) ~ 0)

# creating school id variable for school student was in in their first year of STAR

star$schid_yr1star <- case_when(star$flagsgk == 1 ~ star$gkschid,
                                ((star$flagsgk == 0) & (star$flagsg1 == 1)) ~ star$g1schid,
                                ((star$flagsgk == 0) & (star$flagsg1 == 0) & (star$flagsg2 == 1)) ~ star$g2schid,
                                ((star$flagsgk == 0) & (star$flagsg1 == 0) & (star$flagsg2 == 0) & (star$flagsg3 == 1)) ~ star$g3schid)

# Load K-3 school level data
k3_schools <- read_sav("~/Documents/Stanford/ECON 293 -  Machine Learning & Causal Inference/Econ293_Final_Project/Untitled/data/STAR_K-3_Schools.sav")

k3_schools <- k3_schools %>% clean_names()

# create a duplicate of school data to merge into student data for each year of k-3
k3_k_schools <- k3_schools


colnames(k3_k_schools) <- paste(colnames(k3_k_schools), "k", sep = "_")

k3_g1_schools <- k3_schools

colnames(k3_g1_schools) <- paste(colnames(k3_g1_schools), "g1", sep = "_")

k3_g2_schools <- k3_schools

colnames(k3_g2_schools) <- paste(colnames(k3_g2_schools), "g2", sep = "_")

k3_g3_schools <- k3_schools

colnames(k3_g3_schools) <- paste(colnames(k3_g3_schools), "g3", sep = "_")

# Merge-in school data separately for each grade the student was in K-3 

star <- star %>% 
  left_join(k3_k_schools, by = c("gkschid" = "schid_k")) %>% 
  left_join(k3_g1_schools, by = c("g1schid" = "schid_g1")) %>% 
  left_join(k3_g2_schools, by = c("g2schid" = "schid_g2")) %>% 
  left_join(k3_g3_schools, by = c("g3schid" = "schid_g3"))
  
# Drop observations which have key variables missing
na_counts = map(star, ~sum(is.na(.)))


#SHORT TERM OUTCOMES
# Define short-term: grade 3 outcomes
short_term_outcomes <- c("g1treadss", "g1tmathss", "g1wordskillss")


# List of variables to drop whe predicting short-term outcomes
drop_for_short_term <- c("g3tlistss", "g3sciencess", "g3socialsciss", "g3spellss",
                         "g3vocabss", "g3mathcomputss", "g3mathnumconcss", 
                         "g3mathapplss", "g3wordskillss", "g3mathbsobjraw", 
                         "g3mathbsobjpct", "g3readbsobjpct")

# Drop the other grade 3 outcomes from the list above
star_short_term <- star[,!(names(star) %in% drop_for_short_term)]

# Drop future (beyond grade 3) outcomes and characteristics
star_short_term <- star_short_term %>% 
  select(!starts_with(c("g4", "g5", "g6", "g7", "g8" , "hs")))

# LONG TERM OUTCOMES
# 7TH GRADE
# Define one long term outcome: 7th grade outcomes
g7_outcomes <- c("g7treadss", "g7tmathss")

# List of variables to drop when predicting 7th grade outcomes
drop_for_g7 <- c("g7tlangss","g7tbattss", "g7sciencess", "g7socialsciss",
                 "g7readcomprehss", "g7spellss", "g7vocabss", "g7mathcomputss",
                 "g7mathconcapplss", "g7langexpss", "g7langmechss", 
                 "g7studyskillss", "g7readbsobjraw", "g7mathbsobjraw")

# Drop the other grade 7 outcomes from the list above
star_g7 <- star[,!(names(star) %in% drop_for_g7)]

# Drop future (beyond grade 7) outcomes and characteristics)
star_g7 <- star_g7 |>
  select(!starts_with(c("g8", "hs")))

# high school GPA
hs_outcomes <- c("hsgpaoverall")

#drop other HS GPA outcomes
drop_for_hs <- c("hsgpaflang", "hsgpamath", "hsgpascience")

# Drop the other HS GPA outcomes from the list above
star_hs <- star[,!(names(star) %in% drop_for_hs)]
```

# Short-Term Effects Analysis

## 1st grade

### Cleaning

```{r g3 math cleaning, warning=FALSE}

star_short_term <- star_short_term %>% 
  filter(!is.na(g1_smallclass)) %>% 
  mutate(W = if_else(g1_smallclass==1, 1, 0))
  
# Drop small class size flags
star_short_term <- star_short_term %>% 
  dplyr::select(-gkclasstype, -g1classtype, -g2classtype, -g3classtype, -cmpstype,
                -yearssmall, -gk_smallclass, -g1_smallclass, -g2_smallclass, -g3_smallclass,
                -yearsstar, -g1readbsraw, -g1mathbsraw, -g1readbsobjraw, -g1mathbsobjraw,
                -g1readbsobjpct, -g1mathbsobjpct, -g1tchid, -gktchid, -g1promote, -cmpsdura)

# star_short_term_1 <- star_short_term %>%
#   filter(!is.na(g3tmathss)) %>%
#   dplyr::select(-(ends_with("_k") | ends_with("_g1") | ends_with("_g2"))) 

# Exclude school level characteristics
star_short_term_1 <- star_short_term %>%
  dplyr::select(-(ends_with("_k") | ends_with("_g1") | ends_with("_g2") | ends_with("_g3")))

star_short_term_1 <- star_short_term_1 %>%
  dplyr::select(-(starts_with("g2") | starts_with("g3")))

std_err <- function(x) sd(x)/sqrt(length(x))
```

### Math and Reading ATE via linear regression and causal forest

```{r g1 outcomes}

i <- 0 
for (outcome in short_term_outcomes) {
  
  # Drop observations with missing values for the outcome
  star_short_term_1 <- star_short_term_1 %>%
    filter(!is.na(get(outcome)))
  
  # Define outcome and treatment vectors
  W <- star_short_term_1$W
  
  Y <- star_short_term_1 %>% pull(outcome) %>% as.vector()
  
  # Drop other covariates that are not useful
  star_short_term_x <- star_short_term_1 %>% 
    dplyr::select(-W, -stdntid) %>% 
    dplyr::select(-contains(c(short_term_outcomes))) %>% 
    dplyr::select(-contains("classsize")) %>% 
    dplyr::select(-contains("flag")) %>% 
    dplyr::select(-gkschid, -schid_yr1star)
  
  n <- nrow(star_short_term_1)
  
  
  # Linear Regression -------------------------------------------------------
  
  # Define a list of covariates manually inputed
  covariates <- c("race", "gender", "birthyear", "g1surban", "g2surban",
                  "g2readbsraw", "g2motivraw", "g3surban",
                  "g3freelunch", "g3schid", "g3tyears", "g2tyear")
  
  star_short_term_cov <- star_short_term_x %>% 
    dplyr::select(contains(c(covariates))) 
  
  
  linear_model_data <- star_short_term_x %>%
    mutate(W = W,
           y = Y) %>%
    dplyr::select(W, y, race, gender, birthyear, g1surban,
                  g1freelunch, g1schid)
  
  fe_model <- plm(y ~ factor(W) + factor(race) + gender + g1surban + g1freelunch,
                  data = linear_model_data,
                  index = c("g1schid"),
                  model = "within")
  
  # print summary using robust standard errors
  print(coeftest(fe_model, vcov. = vcovHC, type = "HC1"))
  
  
  # Causal Forest -----------------------------------------------------------
  
  # Estimate a causal forest
  forest <- causal_forest(X = star_short_term_x, Y=Y, W=W, 
                          clusters = star_short_term_x$g1schid,
                          min.node.size = 5,
                          tune.parameters = "all",
                          seed = 20220524)
  
  # Plot propensity scores
  # hist(forest$W.hat)
  
  # Average treatment effect
  print(outcome)
  ate <- average_treatment_effect(forest, method = "AIPW")
  print(ate)
  print(paste0("95% CI: ", round(ate[1],2),
             " +/- ", round(1.96 * ate[2], 2)))
  
  # Explore variable importance suggested by the causal forest
   var_importance <- variable_importance(forest) %>% 
     as_tibble() %>% 
     mutate(n_col = row_number()) %>% 
     arrange(-V1)
   
   important_vars <- var_importance %>% 
     slice(1:10) %>% 
     pull(n_col)          
   
   important_vars_names <- names(star_short_term_x[important_vars])
   print(important_vars_names)
  
  star_short_term_x <- star_short_term_x %>% 
    mutate(cate_estimate = predict(forest)$predictions)
  
  titles <- c("Reading", "Math", "Word Skills")
  i <- i + 1
  
  star_short_term_x %>% 
    ggplot(aes(x = cate_estimate)) +
    geom_histogram(bins = 50, color = "black") + 
    theme_light() +
    xlab("CATE Estimates") +
    ylab("Number of students") +
    geom_vline(aes(xintercept=ate[1]), color="cyan") +
    geom_text(aes(x=(ate[1] - 1), label="AIPW ATE", y=310), color='cyan', size = 12) +
    ggtitle(paste("Histogram of 1st Grade", titles[i], "CATE Estimates")) +
  theme(plot.title = element_text(hjust = 0.5, size = 30),
        axis.text = element_text(size=16),
        axis.title = element_text(size = 20),
        legend.text = element_text(size = 20)) 
  
   ggsave(glue("/Users/lacm/Documents/Stanford/ECON 293 -  Machine Learning & Causal Inference/Econ293_Final_Project/cate_estimates_hist_short_term_{outcome}.png"),
         width = 14,
         height = 10)
  
  
  # ggplot(star_short_term_x, aes(race, g1freelunch, fill = cate_estimate)) +
  #   geom_raster()
  # 
  # star_short_term_x %>% 
  #   # group_by(race) %>% 
  #   # summarize(mean_cate = mean(cate_estimate)) %>% 
  #   # ungroup() %>% 
  #   ggplot(aes(x = factor(g1surban), y = cate_estimate)) +
  #   geom_boxplot(aes(group = factor(g1surban)))
  
  
  save(star_short_term_x, file = glue("./data/cate_estimates_short_term_{outcome}.rda"))


# Analyzing CATE estimates ------------------------------------------------


# Test for heterogeneous treatment effects
  test_calibration(forest)
  
  quartile_est_df <- tibble(quartile = rep(c(1, 2, 3, 4), 2), 
                            type = c(rep("Mean CATE", 4), rep("AIPW ATE", 4)),
                            estimate = rep(NA_real_, 8),
                            std_err = rep(NA_real_, 8))
  
  
  star_short_term_x <- star_short_term_x %>% 
    mutate(
      y = Y,
      cate_quartile = ntile(cate_estimate, 4),
      w = W
    )
  
  # Compute ATE using AIPW for each quartile
  for (q in seq(1, 4, 1)) {
    
    X_q = star_short_term_x %>%
      filter(cate_quartile==q) %>% 
      dplyr::select(-cate_quartile, -cate_estimate, -y, -w) 
    
    Y_q <- star_short_term_x %>%
      filter(cate_quartile==q) %>% 
      pull(y)
    
    W_q <- star_short_term_x %>%
      filter(cate_quartile==q) %>% 
      pull(w)
    
    g1_school <- star_short_term_x %>%
      filter(cate_quartile==q) %>%
      pull(g1schid)
    
    forest_q <- causal_forest(X_q, Y_q, W_q,
                              clusters = g1_school,
                              tune.parameters = "all",
                              seed = 20220524)
    
    quartile_ate <- average_treatment_effect(forest_q, target.sample = "overlap")
    
    aipw_se = quartile_ate[2]
    
    # print(quartile_ate)
    
    mean_cate = star_short_term_x %>%
      filter(cate_quartile==q) %>% 
      pull(cate_estimate) %>% 
      mean()
    
    cate_se = star_short_term_x %>%
      filter(cate_quartile==q) %>% 
      pull(cate_estimate) %>% 
      std_err()
    
    quartile_est_df[q, "estimate"] <- mean_cate
    quartile_est_df[q, "std_err"] <- cate_se
    quartile_est_df[q+4, "estimate"] <- quartile_ate[1]
    quartile_est_df[q+4, "std_err"] <- aipw_se
    
    # print(mean_cate)
    
  }
  
  ggplot(quartile_est_df) +
    aes(x = quartile, y = estimate, group=type, color=type) + 
    geom_point(position=position_dodge(0.2), size = 3) +
    geom_errorbar(aes(ymin=estimate-2*std_err, ymax=estimate+2*std_err),
                  width=.2, position=position_dodge(0.2),
                  size = 1.2) +
    ylab("") + xlab("CATE Quartile") +
    ggtitle(paste("1st Grade", titles[i], "CATE Estimates by Quartile")) +
    theme_light() +
    theme(legend.position="bottom", 
          legend.title = element_blank(), 
          plot.title = element_text(hjust = 0.5, size = 30),
          axis.text = element_text(size=16),
          axis.title = element_text(size = 20),
          legend.text = element_text(size = 20))  
    
   ggsave(glue("/Users/lacm/Documents/Stanford/ECON 293 -  Machine Learning & Causal Inference/Econ293_Final_Project/cate_quartiles_short_term_{outcome}.png"),
         width = 14,
         height = 10)

   #heterogeneity by free lunch status plots 
   
  print(star_short_term_x %>% 
    filter(!is.na(g1freelunch)) %>%
    ggplot(aes(x = factor(g1freelunch), y = cate_estimate, fill = factor(g1freelunch))) +
    geom_boxplot() + 
    theme_light() +
    xlab("Student Receives Free Lunch") +
    ylab("CATE Estimates") +
    ggtitle(paste("1st Grade", titles[i], "CATE Estimates by Student Free Lunch Status")) +
  theme(plot.title = element_text(hjust = 0.5, size = 30),
        axis.text = element_text(size=16),
        axis.title = element_text(size = 20),
        legend.text = element_text(size = 20),
        legend.position = "none") +
    scale_x_discrete(breaks=c("1", "2"),
                      labels=c("Yes", "No")))
  ggsave(glue("/Users/lacm/Documents/Stanford/ECON 293 -  Machine Learning & Causal Inference/Econ293_Final_Project/cate_freelunch_short_term_{outcome}.png"),
         width = 14,
         height = 10)
  
  #heterogeneity by school urbanicity plots 
   
  print(star_short_term_x %>% 
    ggplot(aes(x = factor(g1surban), y = cate_estimate, fill = factor(g1surban))) +
    geom_boxplot() + 
    theme_light() +
    xlab("Urbanicity of Student's School") +
    ylab("CATE Estimates") +
    ggtitle(paste("1st Grade", titles[i], "CATE Estimates by Urbanicity of Student's School")) +
  theme(plot.title = element_text(hjust = 0.5, size = 30),
        axis.text = element_text(size=16),
        axis.title = element_text(size = 20),
        legend.text = element_text(size = 20),
        legend.position = "none") +
    scale_x_discrete(breaks=c("1", "2", "3", "4"),
                      labels=c("Inner City", "Suburban", "Rural", "Urban")))
  
  ggsave(glue("/Users/lacm/Documents/Stanford/ECON 293 -  Machine Learning & Causal Inference/Econ293_Final_Project/cate_urbanicity_short_term_{outcome}.png"),
         width = 14,
         height = 10)
   
  #heterogeneity by race plots 
   
  print(star_short_term_x %>% 
    filter(!is.na(race)) %>%
    ggplot(aes(x = factor(race), y = cate_estimate, fill = factor(race))) +
    geom_boxplot() + 
    theme_light() +
    xlab("Student's Race") +
    ylab("CATE Estimates") +
    ggtitle(paste("1st Grade", titles[i], "CATE Estimates by Students' Race")) +
  theme(plot.title = element_text(hjust = 0.5, size = 30),
        axis.text = element_text(size=16),
        axis.title = element_text(size = 20),
        legend.text = element_text(size = 20),
        legend.position = "none") +
    scale_x_discrete(breaks=c("1", "2", "3", "4", "5", "6"),
                      labels=c("White", "Black", "Asian", "Hispanic", "Native American", "Other")))
  
  ggsave(glue("/Users/lacm/Documents/Stanford/ECON 293 -  Machine Learning & Causal Inference/Econ293_Final_Project/cate_race_short_term_{outcome}.png"),
         width = 14,
         height = 10)
}

```

# Long-Term Effects Analysis

## 7th grade math

### Cleaning

```{r g7 math cleaning, warning=FALSE}

star_g7_math <- star_g7 |>
  filter(!is.na(g7tmathss))

# Define treatment and outcome vectors
W <- star_g7_math$W

outcome <- g7_outcomes[2]

Y <- star_g7_math |>
  pull(outcome) |>
  as.vector()

star_g7_x <- star_g7_math |>
  select(-W, -stdntid, -gkclasstype, -g1classtype, -g2classtype, -g3classtype, -cmpstype,-cmpsdura, -yearssmall, -yearsstar, -gk_smallclass, -g1_smallclass, -g2_smallclass, -g3_smallclass) |>
  select(!starts_with(c("birth", "flag"))) |>
  select(!ends_with(c("schid", "promote", "_k", "_g1", "_g2", "_g3", "tchid", "classsize"))) |>
  select(!contains(c(g7_outcomes)))

```

### ATE via linear regression

```{r g7 math linear regression, warning=FALSE}
#primary analysis
linear_regression <- plm(g7tmathss ~ factor(W) + factor(race) + gender + gksurban + g1surban + g2surban + g3surban + gkfreelunch + g1freelunch + g2freelunch + g3freelunch,
                  data = star_g7_math,
                  index = c("schid_yr1star"),
                  model = "within")
  
# print summary using robust standard errors
coeftest(linear_regression, vcov. = vcovHC, type = "HC1")


#other exploratory analyses
bivariate_ols <- lm(g7tmathss ~ W 
                    + as.factor(gkschid)
                    + as.factor(g1schid)
                    + as.factor(g2schid)
                    + as.factor(g3schid)
                    + as.factor(g4schid)
                    + as.factor(g5schid)
                    + as.factor(g6schid)
                    + as.factor(g7schid), data=star_g7_math)
summary(bivariate_ols)


yearssmall_ols <- lm(g7tmathss ~ as.factor(yearssmall) 
                     + as.factor(gkschid)
                     + as.factor(g1schid)
                     + as.factor(g2schid)
                     + as.factor(g3schid)
                     + as.factor(g4schid)
                     + as.factor(g5schid)
                     + as.factor(g6schid)
                     + as.factor(g7schid), data=star_g7_math)
summary(yearssmall_ols)

ksmall_ols <- lm(g7tmathss ~ gk_smallclass
                 + as.factor(gkschid)
                 + as.factor(g1schid)
                 + as.factor(g2schid)
                 + as.factor(g3schid)
                 + as.factor(g4schid)
                 + as.factor(g5schid)
                 + as.factor(g6schid)
                 + as.factor(g7schid), data=star_g7_math)
summary(ksmall_ols)

g1small_ols <- lm(g7tmathss ~ g1_smallclass
                  + as.factor(gkschid)
                  + as.factor(g1schid)
                  + as.factor(g2schid)
                  + as.factor(g3schid)
                  + as.factor(g4schid)
                  + as.factor(g5schid)
                  + as.factor(g6schid)
                  + as.factor(g7schid), data=star_g7_math)
summary(g1small_ols)

g2small_ols <- lm(g7tmathss ~ g2_smallclass + as.factor(gkschid)
                  + as.factor(g1schid)
                  + as.factor(g2schid)
                  + as.factor(g3schid)
                  + as.factor(g4schid)
                  + as.factor(g5schid)
                  + as.factor(g6schid)
                  + as.factor(g7schid)
                  , data=star_g7_math)
summary(g2small_ols)

g3small_ols <- lm(g7tmathss ~ g3_smallclass 
                  + as.factor(gkschid)
                  + as.factor(g1schid)
                  + as.factor(g2schid)
                  + as.factor(g3schid)
                  + as.factor(g4schid)
                  + as.factor(g5schid)
                  + as.factor(g6schid)
                  + as.factor(g7schid), data=star_g7_math)
summary(g3small_ols)
```

### ATE via Causal Forest/AIPW

```{r g7 math aipw, warning=FALSE}

n <-nrow(star_g7_math)

# Number of rankings predictions will be ranked on
num.rankings <- 5

# Prepare for data.splitting
# Assign a fold number to each observation.
# The argument 'clusters' in the next step will mimic K-fold cross-fitting.
num.folds <- 10
folds <- sort(seq(n) %% num.folds) + 1

# Estimate a causal forest
forest <- causal_forest(X=star_g7_x, Y=Y, W=W, W.hat=mean(W),
                        clusters = star_g7_math$schid_yr1star,
                        min.node.size = 5,
                        tune.parameters = "all",
                        seed = 20220524)

#histogram of estimated HTE via causal forest
hist(predict(forest)$predictions)

#save estimates to dataframe
star_g7_math_hte <- star_g7_math
star_g7_math_hte$hte <- forest$predictions


#estimated ATE
g7_ate_math <- average_treatment_effect(forest, target.sample = "overlap", method = "AIPW")
g7_ate_math_se <- g7_ate_math[2]
g7_ate_math <- g7_ate_math[1]

print(paste0("95% CI: ", round(g7_ate_math,2),
             " +/- ", round(1.96 * g7_ate_math_se, 2)))

#important contributing variables in causal forest
var_importance <- variable_importance(forest) |> 
  as_tibble() |>
  mutate(n_col = row_number()) |>
  arrange(-V1)

important_vars_g7_math <- var_importance %>% 
  slice(1:10) %>% 
  pull(n_col)

important_vars_g7_math <- names(star_g7_x[important_vars_g7_math])
important_vars_g7_math
```

### HTE via Causal Forest

```{r g7 math hte}
g7_math_hte <- test_calibration(forest)
g7_math_hte
```

### Visualizations

```{r g7 math visualizations, warning=FALSE}

#heterogeneity by free lunch status plots 
   
  star_g7_math_hte %>% 
    filter(!is.na(g3freelunch)) %>%
    ggplot(aes(x = factor(g3freelunch), y = hte, fill = factor(g3freelunch))) +
    geom_boxplot() + 
    theme_light() +
    xlab("Student Receives Free Lunch") +
    ylab("CATE Estimates") +
    ggtitle(paste("7th Grade Math CATE Estimates by Student Free Lunch Status")) +
  theme(plot.title = element_text(hjust = 0.5, size = 30),
        axis.text = element_text(size=16),
        axis.title = element_text(size = 20),
        legend.text = element_text(size = 20),
        legend.position = "none") +
    scale_x_discrete(breaks=c("1", "2"),
                      labels=c("Yes", "No"))
  
  ggsave(glue("/Users/lacm/Documents/Stanford/ECON 293 -  Machine Learning & Causal Inference/Econ293_Final_Project/cate_freelunch_g7_math.png"),
         width = 14,
         height = 10)
  
  #heterogeneity by school urbanicity plots 
   
  star_g7_math_hte %>% 
    filter(!is.na(g3surban)) %>%
    ggplot(aes(x = factor(g3surban), y = hte, fill = factor(g3surban))) +
    geom_boxplot() + 
    theme_light() +
    xlab("Urbanicity of Student's School") +
    ylab("CATE Estimates") +
    ggtitle(paste("7th Grade Math CATE Estimates by Urbanicity of Student's School")) +
  theme(plot.title = element_text(hjust = 0.5, size = 30),
        axis.text = element_text(size=16),
        axis.title = element_text(size = 20),
        legend.text = element_text(size = 20),
        legend.position = "none") +
    scale_x_discrete(breaks=c("1", "2", "3", "4"),
                      labels=c("Inner City", "Suburban", "Rural", "Urban"))
  
  ggsave(glue("/Users/lacm/Documents/Stanford/ECON 293 -  Machine Learning & Causal Inference/Econ293_Final_Project/cate_urbanicity_g7_math.png"),
         width = 14,
         height = 10)
  
   #heterogeneity by race plots
  
  star_g7_math_hte %>% 
    filter(!is.na(race)) %>%
    ggplot(aes(x = factor(race), y = hte, fill = factor(race))) +
    geom_boxplot() + 
    theme_light() +
    xlab("Student's Race") +
    ylab("CATE Estimates") +
    ggtitle(paste("7th Grade Math CATE Estimates by Students' Race")) +
  theme(plot.title = element_text(hjust = 0.5, size = 30),
        axis.text = element_text(size=16),
        axis.title = element_text(size = 20),
        legend.text = element_text(size = 20),
        legend.position = "none") +
    scale_x_discrete(breaks=c("1", "2", "3", "4", "5", "6"),
                      labels=c("White", "Black", "Asian", "Hispanic", "Native American", "Other"))
  
  ggsave(glue("/Users/lacm/Documents/Stanford/ECON 293 -  Machine Learning & Causal Inference/Econ293_Final_Project/cate_race_g7_math.png"),
         width = 14,
         height = 10)
```

## 7th grade reading

### Cleaning

```{r g7 reading cleaning, warning=FALSE}

star_g7_reading <- star_g7 |>
  filter(!is.na(g7treadss))

# Define treatment and outcome vectors
W <- star_g7_reading$W

outcome <- g7_outcomes[1]

Y <- star_g7_reading |>
  pull(outcome) |>
  as.vector()

star_g7_x <- star_g7_reading |>
  select(-W, -stdntid, -gkclasstype, -g1classtype, -g2classtype, -g3classtype, -cmpstype,-cmpsdura, -yearssmall, -yearsstar, -gk_smallclass, -g1_smallclass, -g2_smallclass, -g3_smallclass) |>
  select(!starts_with(c("birth", "flag"))) |>
  select(!ends_with(c("schid", "promote", "_k", "_g1", "_g2", "_g3", "tchid", "classsize"))) |>
  select(!contains(c(g7_outcomes)))
```

### ATE via linear regression

```{r g7 reading linear regression, warning=FALSE}

# primary analysis
linear_regression <- plm(g7treadss ~ factor(W) + factor(race) + gender + gksurban + g1surban + g2surban + g3surban + gkfreelunch + g1freelunch + g2freelunch + g3freelunch,
                  data = star_g7_reading,
                  index = c("schid_yr1star"),
                  model = "within")

# print summary using robust standard errors
coeftest(linear_regression, vcov. = vcovHC, type = "HC1")


#exploratory analyses
bivariate_ols <- lm(g7treadss ~ W 
                    + as.factor(gkschid)
                    + as.factor(g1schid)
                    + as.factor(g2schid)
                    + as.factor(g3schid)
                    + as.factor(g4schid)
                    + as.factor(g5schid)
                    + as.factor(g6schid)
                    + as.factor(g7schid), data=star_g7_reading)
summary(bivariate_ols)

yearssmall_ols <- lm(g7treadss ~ as.factor(yearssmall) + as.factor(gkschid)
                     + as.factor(g1schid)
                     + as.factor(g2schid)
                     + as.factor(g3schid)
                     + as.factor(g4schid)
                     + as.factor(g5schid)
                     + as.factor(g6schid)
                     + as.factor(g7schid), data=star_g7_reading)
summary(yearssmall_ols)

ksmall_ols <- lm(g7treadss ~ gk_smallclass + as.factor(gkschid)
                 + as.factor(g1schid)
                 + as.factor(g2schid)
                 + as.factor(g3schid)
                 + as.factor(g4schid)
                 + as.factor(g5schid)
                 + as.factor(g6schid)
                 + as.factor(g7schid), data=star_g7_reading)
summary(ksmall_ols)

g1small_ols <- lm(g7treadss ~ g1_smallclass + as.factor(gkschid)
                  + as.factor(g1schid)
                  + as.factor(g2schid)
                  + as.factor(g3schid)
                  + as.factor(g4schid)
                  + as.factor(g5schid)
                  + as.factor(g6schid)
                  + as.factor(g7schid), data=star_g7_reading)
summary(g1small_ols)

g2small_ols <- lm(g7treadss ~ g2_smallclass + as.factor(gkschid)
                  + as.factor(g1schid)
                  + as.factor(g2schid)
                  + as.factor(g3schid)
                  + as.factor(g4schid)
                  + as.factor(g5schid)
                  + as.factor(g6schid)
                  + as.factor(g7schid), data=star_g7_reading)
summary(g2small_ols)

g3small_ols <- lm(g7treadss ~ g3_smallclass + as.factor(gkschid)
                  + as.factor(g1schid)
                  + as.factor(g2schid)
                  + as.factor(g3schid)
                  + as.factor(g4schid)
                  + as.factor(g5schid)
                  + as.factor(g6schid)
                  + as.factor(g7schid), data=star_g7_reading)
summary(g3small_ols)
```

### ATE via Causal Forest/AIPW

```{r g7 reading aipw, warning=FALSE}

n <-nrow(star_g7_reading)

# Number of rankings predictions will be ranked on
num.rankings <- 5

# Prepare for data.splitting
# Assign a fold number to each observation.
# The argument 'clusters' in the next step will mimic K-fold cross-fitting.
num.folds <- 10
folds <- sort(seq(n) %% num.folds) + 1

# Estimate a causal forest
forest <- causal_forest(X = star_g7_x, Y=Y, W=W,W.hat=mean(W),
                        clusters = star_g7_reading$schid_yr1star,
                        min.node.size = 5,
                        tune.parameters = "all",
                        seed = 20220524)

#histogram of estimated heterogeneous effects
hist(predict(forest)$predictions)


#saving HTEs to dataframe
star_g7_reading_hte <- star_g7_reading
star_g7_reading_hte$hte <- forest$predictions

#estimated ATEs
g7_ate_reading <- average_treatment_effect(forest, target.sample = "overlap", method = "AIPW")
g7_ate_reading_se <- g7_ate_reading[2]
g7_ate_reading <- g7_ate_reading[1]

print(paste0("95% CI: ", round(g7_ate_reading,2),
             " +/- ", round(1.96 * g7_ate_reading_se, 2)))

#imortant contributing variables in causal forest
var_importance <- variable_importance(forest) |> 
  as_tibble() |>
  mutate(n_col = row_number()) |>
  arrange(-V1)

important_vars_g7_reading <- var_importance %>% 
  slice(1:10) %>% 
  pull(n_col)

important_vars_g7_reading<- names(star_g7_x[important_vars_g7_reading])
important_vars_g7_reading
```

### HTE via Causal Forest

```{r g7 reading hte, warning=FALSE}
g7_reading_hte <- test_calibration(forest)
g7_reading_hte
```

### Visualizations

```{r g7 reading visualizations, warning=FALSE}

#heterogeneity by free lunch status plots 
   
  star_g7_reading_hte %>% 
    filter(!is.na(g3freelunch)) %>%
    ggplot(aes(x = factor(g3freelunch), y = hte, fill = factor(g3freelunch))) +
    geom_boxplot() + 
    theme_light() +
    xlab("Student Receives Free Lunch") +
    ylab("CATE Estimates") +
    ggtitle(paste("7th Grade Reading CATE Estimates by Student Free Lunch Status")) +
  theme(plot.title = element_text(hjust = 0.5, size = 30),
        axis.text = element_text(size=16),
        axis.title = element_text(size = 20),
        legend.text = element_text(size = 20),
        legend.position = "none") +
    scale_x_discrete(breaks=c("1", "2"),
                      labels=c("Yes", "No"))
  
  ggsave(glue("/Users/lacm/Documents/Stanford/ECON 293 -  Machine Learning & Causal Inference/Econ293_Final_Project/cate_freelunch_g7_reading.png"),
         width = 14,
         height = 10)
  
  #heterogeneity by school urbanicity plots 
   
  star_g7_reading_hte %>% 
    filter(!is.na(g3surban)) %>%
    ggplot(aes(x = factor(g3surban), y = hte, fill = factor(g3surban))) +
    geom_boxplot() + 
    theme_light() +
    xlab("Urbanicity of Student's School") +
    ylab("CATE Estimates") +
    ggtitle(paste("7th Grade Reading CATE Estimates by Urbanicity of Student's School")) +
  theme(plot.title = element_text(hjust = 0.5, size = 30),
        axis.text = element_text(size=16),
        axis.title = element_text(size = 20),
        legend.text = element_text(size = 20),
        legend.position = "none") +
    scale_x_discrete(breaks=c("1", "2", "3", "4"),
                      labels=c("Inner City", "Suburban", "Rural", "Urban"))
  
  ggsave(glue("/Users/lacm/Documents/Stanford/ECON 293 -  Machine Learning & Causal Inference/Econ293_Final_Project/cate_urbanicity_g7_reading.png"),
         width = 14,
         height = 10)
  
  
  #heterogeneity by race plots
  
  star_g7_reading_hte %>% 
    filter(!is.na(race)) %>%
    ggplot(aes(x = factor(race), y = hte, fill = factor(race))) +
    geom_boxplot() + 
    theme_light() +
    xlab("Student's Race") +
    ylab("CATE Estimates") +
    ggtitle(paste("7th Grade Reading CATE Estimates by Students' Race")) +
  theme(plot.title = element_text(hjust = 0.5, size = 30),
        axis.text = element_text(size=16),
        axis.title = element_text(size = 20),
        legend.text = element_text(size = 20),
        legend.position = "none") +
    scale_x_discrete(breaks=c("1", "2", "3", "4", "5", "6"),
                      labels=c("White", "Black", "Asian", "Hispanic", "Native American", "Other"))
  
  ggsave(glue("/Users/lacm/Documents/Stanford/ECON 293 -  Machine Learning & Causal Inference/Econ293_Final_Project/cate_race_g7_reading.png"),
         width = 14,
         height = 10)
```

## High School GPA

### Cleaning

```{r hs cleaning, warning=FALSE}
star_hs_1 <- star_hs |>
  filter(!is.na(hsgpaoverall))

# Define treatment and outcome vectors
W <- star_hs_1$W

outcome <- hs_outcomes[1]

Y <- star_hs_1 |>
  pull(outcome) |>
  as.vector()

star_hs_x <- star_hs_1 |>
  select(-W, -stdntid, -gkclasstype, -g1classtype, -g2classtype, -g3classtype, -cmpstype,-cmpsdura, -yearssmall, -yearsstar, -gk_smallclass, -g1_smallclass, -g2_smallclass, -g3_smallclass) |>
  select(!starts_with(c("birth", "flag"))) |>
  select(!ends_with(c("schid", "promote", "_k", "_g1", "_g2", "_g3", "tchid", "classsize")))
```

### ATE via linear regression

```{r hs linear regression, warning=FALSE}

# primary analysis
linear_regression <- plm(hsgpaoverall ~ factor(W) + factor(race) + gender,
                  data = star_hs_1,
                  index = c("schid_yr1star"),
                  model = "within")

# print summary using robust standard errors
coeftest(linear_regression, vcov. = vcovHC, type = "HC1")


#exploratory analyses
bivariate_ols <- lm(hsgpaoverall ~ W + as.factor(gkschid)
                    + as.factor(g1schid)
                    + as.factor(g2schid)
                    + as.factor(g3schid)
                    + as.factor(g4schid)
                    + as.factor(g5schid)
                    + as.factor(g6schid)
                    + as.factor(g7schid)
                    + as.factor(g8schid)
                    , data=star_hs_1)
summary(bivariate_ols)


yearssmall_ols <- lm(hsgpaoverall ~ as.factor(yearssmall) 
                     + as.factor(gkschid)
                     + as.factor(g1schid)
                     + as.factor(g2schid)
                     + as.factor(g3schid)
                     , data=star_hs_1)
summary(yearssmall_ols)

ksmall_ols <- lm(hsgpaoverall ~ gk_smallclass +as.factor(gkschid)
                 , data=star_hs_1)
summary(ksmall_ols)

g1small_ols <- lm(hsgpaoverall ~ g1_smallclass 
                  + as.factor(gkschid)
                  + as.factor(g1schid)
                  , data=star_hs_1)
summary(g1small_ols)

g2small_ols <- lm(hsgpaoverall ~ g2_smallclass+as.factor(gkschid)
                  + as.factor(g1schid)
                  + as.factor(g2schid)
                  , data=star_hs_1)
summary(g2small_ols)

g3small_ols <- lm(hsgpaoverall ~ g3_smallclass + as.factor(gkschid)
                  + as.factor(g1schid)
                  + as.factor(g2schid)
                  + as.factor(g3schid)
                  , data=star_hs_1)
summary(g3small_ols)
```

### ATE via Causal Forest/AIPW

```{r hs aipw, warning=FALSE}

n <- nrow(star_hs_1)

# Number of rankings predictions will be ranked on
num.rankings <- 5

# Prepare for data.splitting
# Assign a fold number to each observation.
# The argument 'clusters' in the next step will mimic K-fold cross-fitting.
num.folds <- 10
folds <- sort(seq(n) %% num.folds) + 1

# Estimate a causal forest
forest <- causal_forest(X = star_hs_x, Y=Y, W=W, W.hat=mean(W),
                        clusters = star_hs_1$schid_yr1star,
                        min.node.size = 5,
                        tune.parameters = "all",
                        seed = 20220524)

#histogram of estimated HTE via causal forest
hist(predict(forest)$predictions)

#saving HTEs to dataframe
star_hs_1_hte <- star_hs_1
star_hs_1_hte$hte <- forest$predictions

#ate predictions
hs_ate <- average_treatment_effect(forest, target.sample = "overlap", method = "AIPW")
hs_ate_se <- hs_ate[2]
hs_ate <- hs_ate[1]


print(paste0("95% CI: ", round(hs_ate,2),
             " +/- ", round(1.96 * hs_ate_se, 2)))

#important contributing variables in causal forest
var_importance <- variable_importance(forest) |> 
  as_tibble() |>
  mutate(n_col = row_number()) |>
  arrange(-V1)

important_vars_hs <- var_importance %>% 
  slice(1:10) %>% 
  pull(n_col)

important_vars_hs <- names(star_hs_x[important_vars_hs])
important_vars_hs
```

### HTE via Causal Forest

```{r hs hte}
hs_hte <- test_calibration(forest)
hs_hte
```

### Visualizations

```{r hs visualizations, warning=FALSE}

#heterogeneity by free lunch status plots 
   
  star_hs_1_hte %>% 
    filter(!is.na(g3freelunch)) %>%
    ggplot(aes(x = factor(g3freelunch), y = hte, fill = factor(g3freelunch))) +
    geom_boxplot() + 
    theme_light() +
    xlab("Student Receives Free Lunch") +
    ylab("CATE Estimates") +
    ggtitle(paste("High School Composite GPA CATE Estimates\n by Student Free Lunch Status")) +
  theme(plot.title = element_text(hjust = 0.5, size = 30),
        axis.text = element_text(size=16),
        axis.title = element_text(size = 20),
        legend.text = element_text(size = 20),
        legend.position = "none") +
    scale_x_discrete(breaks=c("1", "2"),
                      labels=c("Yes", "No"))
  
  ggsave(glue("/Users/lacm/Documents/Stanford/ECON 293 -  Machine Learning & Causal Inference/Econ293_Final_Project/cate_freelunch_HS.png"),
         width = 14,
         height = 10)
  
  #heterogeneity by school urbanicity plots 
   
  star_hs_1_hte %>% 
    filter(!is.na(g3surban)) %>%
    ggplot(aes(x = factor(g3surban), y = hte, fill = factor(g3surban))) +
    geom_boxplot() + 
    theme_light() +
    xlab("Urbanicity of Student's School") +
    ylab("CATE Estimates") +
    ggtitle(paste("High School Composite GPA CATE Estimates by\n Urbanicity of Student's School")) +
  theme(plot.title = element_text(hjust = 0.5, size = 30),
        axis.text = element_text(size=16),
        axis.title = element_text(size = 20),
        legend.text = element_text(size = 20),
        legend.position = "none") +
    scale_x_discrete(breaks=c("1", "2", "3", "4"),
                      labels=c("Inner City", "Suburban", "Rural", "Urban"))
  
  ggsave(glue("/Users/lacm/Documents/Stanford/ECON 293 -  Machine Learning & Causal Inference/Econ293_Final_Project/cate_urbanicity_HS.png"),
         width = 14,
         height = 10)
  
  
  #heterogeneity by race plots
  
  star_hs_1_hte %>% 
    filter(!is.na(race)) %>%
    ggplot(aes(x = factor(race), y = hte, fill = factor(race))) +
    geom_boxplot() + 
    theme_light() +
    xlab("Student's Race") +
    ylab("CATE Estimates") +
    ggtitle(paste("High School Composite GPA CATE Estimates\n by Students' Race")) +
  theme(plot.title = element_text(hjust = 0.5, size = 30),
        axis.text = element_text(size=16),
        axis.title = element_text(size = 20),
        legend.text = element_text(size = 20),
        legend.position = "none") +
    scale_x_discrete(breaks=c("1", "2", "3", "4", "5", "6"),
                      labels=c("White", "Black", "Asian", "Hispanic", "Native American", "Other"))
  
  ggsave(glue("/Users/lacm/Documents/Stanford/ECON 293 -  Machine Learning & Causal Inference/Econ293_Final_Project/cate_race_hs.png"),
         width = 14,
         height = 10)
```
